---
title: "Create Chat Completion"
description: "Use the POST /v1/chat/completions endpoint to create chat completions, supporting both streaming and non-streaming responses"
---

# Create Chat Completion

```
POST /v1/chat/completions
```

Generates a model response based on the input message list. Supports both streaming and non-streaming response modes.

## Authentication

Pass the API Key in the request header:

```
Authorization: Bearer YOUR_API_KEY
```

## Request Parameters

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `model` | string | Yes | - | Model name, e.g. `gpt-4o`, `claude-sonnet-4-20250514`, `gemini-2.5-flash` |
| `messages` | array | Yes | - | Message list, each message contains `role` and `content` |
| `temperature` | number | No | 1 | Sampling temperature, range 0-2. Higher values produce more random output |
| `top_p` | number | No | 1 | Nucleus sampling parameter, range 0-1. Use either this or temperature |
| `n` | integer | No | 1 | Number of responses to generate |
| `stream` | boolean | No | false | Whether to enable streaming output |
| `stream_options` | object | No | null | Streaming options. Set `{"include_usage": true}` to return usage in the last chunk |
| `stop` | string\|array | No | null | Stop generation tokens, up to 4 |
| `max_tokens` | integer | No | Model default | Maximum number of tokens to generate |
| `presence_penalty` | number | No | 0 | Presence penalty, range -2.0 to 2.0 |
| `frequency_penalty` | number | No | 0 | Frequency penalty, range -2.0 to 2.0 |
| `logprobs` | boolean | No | false | Whether to return logprobs |
| `top_logprobs` | integer | No | null | Number of top logprobs to return, 0-20 |
| `response_format` | object | No | null | Specify output format, e.g. `{"type": "json_object"}` |
| `seed` | integer | No | null | Random seed for reproducible output |
| `tools` | array | No | null | Available tools list (Function Calling) |
| `tool_choice` | string\|object | No | "auto" | Tool selection strategy |
| `user` | string | No | null | End-user identifier |

### messages Format

```json
[
  {"role": "system", "content": "You are a helpful assistant."},
  {"role": "user", "content": "Hello"},
  {"role": "assistant", "content": "Hello! How can I help you?"},
  {"role": "user", "content": "Tell me about yourself"}
]
```

Supported role values: `system`, `user`, `assistant`, `tool`.

---

## Non-Streaming Request

<CodeGroup>

```bash cURL
curl https://crazyrouter.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "gpt-4o",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Explain artificial intelligence in one sentence"}
    ],
    "temperature": 0.7
  }'
```

```python Python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_API_KEY",
    base_url="https://crazyrouter.com/v1"
)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain artificial intelligence in one sentence"}
    ],
    temperature=0.7
)

print(response.choices[0].message.content)
```

```javascript Node.js
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "YOUR_API_KEY",
  baseURL: "https://crazyrouter.com/v1",
});

const response = await client.chat.completions.create({
  model: "gpt-4o",
  messages: [
    { role: "system", content: "You are a helpful assistant." },
    { role: "user", content: "Explain artificial intelligence in one sentence" },
  ],
  temperature: 0.7,
});

console.log(response.choices[0].message.content);
```

</CodeGroup>

### Non-Streaming Response

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1709123456,
  "model": "gpt-4o",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Artificial intelligence is the technology that enables computers to simulate human intelligent behavior, allowing machines to learn, reason, and solve problems."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 28,
    "completion_tokens": 32,
    "total_tokens": 60
  }
}
```

---

## Streaming Request

<CodeGroup>

```bash cURL
curl https://crazyrouter.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "gpt-4o",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Explain artificial intelligence in one sentence"}
    ],
    "stream": true,
    "stream_options": {"include_usage": true}
  }'
```

```python Python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_API_KEY",
    base_url="https://crazyrouter.com/v1"
)

stream = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain artificial intelligence in one sentence"}
    ],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="")
```

```javascript Node.js
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "YOUR_API_KEY",
  baseURL: "https://crazyrouter.com/v1",
});

const stream = await client.chat.completions.create({
  model: "gpt-4o",
  messages: [
    { role: "system", content: "You are a helpful assistant." },
    { role: "user", content: "Explain artificial intelligence in one sentence" },
  ],
  stream: true,
});

for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content || "";
  process.stdout.write(content);
}
```

</CodeGroup>

### Streaming Response

```
data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1709123456,"model":"gpt-4o","choices":[{"index":0,"delta":{"role":"assistant","content":""},"finish_reason":null}]}

data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1709123456,"model":"gpt-4o","choices":[{"index":0,"delta":{"content":"Artificial intelligence"},"finish_reason":null}]}

data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1709123456,"model":"gpt-4o","choices":[{"index":0,"delta":{"content":" is"},"finish_reason":null}]}

data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1709123456,"model":"gpt-4o","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1709123456,"model":"gpt-4o","choices":[],"usage":{"prompt_tokens":28,"completion_tokens":32,"total_tokens":60}}

data: [DONE]
```

<Note>
  When `stream_options.include_usage` is set to `true`, the last chunk will contain the complete `usage` information.
</Note>

<Warning>
  The `max_tokens` parameter limits the number of output tokens, not including input. If the output is truncated, `finish_reason` will return `length`.
</Warning>
